{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "%cd gdrive/MyDrive/federated_project/Federated-Time-Series-Forecasting/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.data_utils import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.models.mlp import MLP\n",
    "from ml.models.rnn import RNN\n",
    "from ml.models.lstm import LSTM\n",
    "from ml.models.gru import GRU\n",
    "from ml.models.cnn import CNN\n",
    "from ml.models.rnn_autoencoder import DualAttentionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.fl.defaults import create_regression_client\n",
    "from ml.fl.client_proxy import SimpleClientProxy\n",
    "from ml.fl.server.server import Server\n",
    "from ml.utils.helpers import accumulate_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(data_path='../dataset/full_dataset.csv', test_size=0.2, targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], num_lags=10, identifier='District', nan_constant=0, x_scaler='minmax', y_scaler='minmax', outlier_detection=True, criterion='mse', fl_rounds=30, fraction=1.0, aggregation='fedavg', epochs=3, lr=0.001, optimizer='adam', batch_size=128, local_early_stopping=False, local_patience=50, max_grad_norm=0.0, reg1=0.0, reg2=0.0, cuda=True, seed=0, assign_stats=None, use_time_features=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    data_path='../dataset/full_dataset.csv', # dataset\n",
    "\n",
    "    test_size=0.2, # validation size \n",
    "    targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], # the target columns\n",
    "    num_lags=10, # the number of past observations to feed as input\n",
    "\n",
    "    identifier='District', # the column name that identifies a bs\n",
    "\n",
    "    nan_constant=0, # the constant to transform nan values\n",
    "    x_scaler='minmax', # x_scaler\n",
    "    y_scaler='minmax', # y_scaler\n",
    "    outlier_detection=True, # whether to perform flooring and capping\n",
    "\n",
    "    criterion='mse', # optimization criterion, mse or l1\n",
    "    fl_rounds=30, # the number of federated rounds\n",
    "    fraction=1., # the percentage of available client to consider for random selection\n",
    "    aggregation=\"fedavg\", # federated aggregation algorithm\n",
    "    epochs=3, # the number of maximum local epochs\n",
    "    lr=0.001, # learning rate\n",
    "    optimizer='adam', # the optimizer, it can be sgd or adam\n",
    "    batch_size=128, # the batch size to use\n",
    "    local_early_stopping=False, # whether to use early stopping\n",
    "    local_patience=50, # patience value for the early stopping parameter (if specified)\n",
    "    max_grad_norm=0.0, # whether to clip grad norm\n",
    "    reg1=0.0, # l1 regularization\n",
    "    reg2=0.0, # l2 regularization\n",
    "\n",
    "    cuda=True, # whether to use gpu\n",
    "    \n",
    "    seed=0, # reproducibility\n",
    "\n",
    "    assign_stats=None, # whether to use statistics as exogenous data, [\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"]\n",
    "    use_time_features=False # whether to use datetime features\n",
    ")\n",
    "\n",
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification\n",
    "if args.outlier_detection is not None:\n",
    "    outlier_columns = ['rb_down', 'rb_up', 'down', 'up']\n",
    "    outlier_kwargs = {\"ElBorn\": (10, 90), \"LesCorts\": (10, 90), \"PobleSec\": (5, 95)}\n",
    "    args.outlier_columns = outlier_columns\n",
    "    args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing():\n",
    "    \"\"\"Preprocess a given .csv\"\"\"\n",
    "    # read data\n",
    "    df = read_data(args.data_path)\n",
    "    # handle nans\n",
    "    df = handle_nans(train_data=df, constant=args.nan_constant,\n",
    "                     identifier=args.identifier)\n",
    "    # split to train/validation\n",
    "    train_data, val_data = to_train_val(df)\n",
    "    \n",
    "    # handle outliers (if specified)\n",
    "    if args.outlier_detection is not None:\n",
    "        train_data = handle_outliers(df=train_data, columns=args.outlier_columns,\n",
    "                                     identifier=args.identifier, kwargs=args.outlier_kwargs)\n",
    "    \n",
    "    # get X and y\n",
    "    X_train, X_val, y_train, y_val = to_Xy(train_data=train_data, val_data=val_data,\n",
    "                                          targets=args.targets)\n",
    "    \n",
    "    # scale X\n",
    "    X_train, X_val, x_scalers = scale_features(train_data=X_train, val_data=X_val,\n",
    "                                              scaler=args.x_scaler,\n",
    "                                              per_area=True, # the features are scaled locally\n",
    "                                              identifier=args.identifier)\n",
    "    # scale y\n",
    "    y_train, y_val, y_scalers = scale_features(train_data=y_train, val_data=y_val,\n",
    "                                              scaler=args.y_scaler, \n",
    "                                              per_area=True,\n",
    "                                              identifier=args.identifier)\n",
    "    \n",
    "    # generate time lags\n",
    "    X_train = generate_time_lags(X_train, args.num_lags)\n",
    "    X_val = generate_time_lags(X_val, args.num_lags)\n",
    "    y_train = generate_time_lags(y_train, args.num_lags, is_y=True)\n",
    "    y_val = generate_time_lags(y_val, args.num_lags, is_y=True)\n",
    "    \n",
    "    # get datetime features as exogenous data\n",
    "    date_time_df_train = time_to_feature(\n",
    "        X_train, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    date_time_df_val = time_to_feature(\n",
    "        X_val, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    \n",
    "    # get statistics as exogenous data\n",
    "    stats_df_train = assign_statistics(X_train, args.assign_stats, args.num_lags,\n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    stats_df_val = assign_statistics(X_val, args.assign_stats, args.num_lags, \n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    \n",
    "    # concat the exogenous features (if any) to a single dataframe\n",
    "    if date_time_df_train is not None or stats_df_train is not None:\n",
    "        exogenous_data_train = pd.concat([date_time_df_train, stats_df_train], axis=1)\n",
    "        # remove duplicate columns (if any)\n",
    "        exogenous_data_train = exogenous_data_train.loc[:, ~exogenous_data_train.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_train) == len(X_train) == len(y_train)\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "    if date_time_df_val is not None or stats_df_val is not None:\n",
    "        exogenous_data_val = pd.concat([date_time_df_val, stats_df_val], axis=1)\n",
    "        exogenous_data_val = exogenous_data_val.loc[:, ~exogenous_data_val.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_val) == len(X_val) == len(y_val)\n",
    "    else:\n",
    "        exogenous_data_val = None\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-02-09 21:49:46,463 | data_utils.py:383 | Observations info in ElBorn\n",
      "INFO logger 2024-02-09 21:49:46,464 | data_utils.py:384 | \tTotal number of samples:  4192\n",
      "INFO logger 2024-02-09 21:49:46,464 | data_utils.py:385 | \tNumber of samples for training: 3354\n",
      "INFO logger 2024-02-09 21:49:46,464 | data_utils.py:386 | \tNumber of samples for validation:  838\n",
      "INFO logger 2024-02-09 21:49:46,466 | data_utils.py:383 | Observations info in LesCorts\n",
      "INFO logger 2024-02-09 21:49:46,466 | data_utils.py:384 | \tTotal number of samples:  6892\n",
      "INFO logger 2024-02-09 21:49:46,466 | data_utils.py:385 | \tNumber of samples for training: 5514\n",
      "INFO logger 2024-02-09 21:49:46,467 | data_utils.py:386 | \tNumber of samples for validation:  1378\n",
      "INFO logger 2024-02-09 21:49:46,471 | data_utils.py:383 | Observations info in PobleSec\n",
      "INFO logger 2024-02-09 21:49:46,471 | data_utils.py:384 | \tTotal number of samples:  15927\n",
      "INFO logger 2024-02-09 21:49:46,472 | data_utils.py:385 | \tNumber of samples for training: 12742\n",
      "INFO logger 2024-02-09 21:49:46,472 | data_utils.py:386 | \tNumber of samples for validation:  3185\n",
      "INFO logger 2024-02-09 21:49:46,479 | data_utils.py:389 | Observations info using all data\n",
      "INFO logger 2024-02-09 21:49:46,480 | data_utils.py:390 | \tTotal number of samples:  27011\n",
      "INFO logger 2024-02-09 21:49:46,480 | data_utils.py:391 | \tNumber of samples for training: 21610\n",
      "INFO logger 2024-02-09 21:49:46,480 | data_utils.py:392 | \tNumber of samples for validation:  5401\n",
      "INFO logger 2024-02-09 21:49:46,481 | data_utils.py:118 | Using Flooring and Capping and with params: {'ElBorn': (10, 90), 'LesCorts': (10, 90), 'PobleSec': (5, 95)}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers = make_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rb_up_var_lag-10</th>\n",
       "      <th>rb_up_lag-10</th>\n",
       "      <th>rb_down_var_lag-10</th>\n",
       "      <th>rb_down_lag-10</th>\n",
       "      <th>mcs_up_var_lag-10</th>\n",
       "      <th>mcs_up_lag-10</th>\n",
       "      <th>mcs_down_var_lag-10</th>\n",
       "      <th>mcs_down_lag-10</th>\n",
       "      <th>rnti_count_lag-10</th>\n",
       "      <th>up_lag-10</th>\n",
       "      <th>...</th>\n",
       "      <th>rb_down_var_lag-1</th>\n",
       "      <th>rb_down_lag-1</th>\n",
       "      <th>mcs_up_var_lag-1</th>\n",
       "      <th>mcs_up_lag-1</th>\n",
       "      <th>mcs_down_var_lag-1</th>\n",
       "      <th>mcs_down_lag-1</th>\n",
       "      <th>rnti_count_lag-1</th>\n",
       "      <th>up_lag-1</th>\n",
       "      <th>down_lag-1</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:16:00</th>\n",
       "      <td>3.143298e-08</td>\n",
       "      <td>0.014949</td>\n",
       "      <td>2.677239e-08</td>\n",
       "      <td>0.116414</td>\n",
       "      <td>0.207425</td>\n",
       "      <td>0.483274</td>\n",
       "      <td>0.796265</td>\n",
       "      <td>0.929664</td>\n",
       "      <td>0.227829</td>\n",
       "      <td>0.043207</td>\n",
       "      <td>...</td>\n",
       "      <td>2.890976e-08</td>\n",
       "      <td>0.145582</td>\n",
       "      <td>0.232918</td>\n",
       "      <td>0.478785</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.916750</td>\n",
       "      <td>0.275640</td>\n",
       "      <td>0.051397</td>\n",
       "      <td>0.288248</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:18:00</th>\n",
       "      <td>4.439640e-08</td>\n",
       "      <td>0.023976</td>\n",
       "      <td>2.795076e-08</td>\n",
       "      <td>0.145097</td>\n",
       "      <td>0.259314</td>\n",
       "      <td>0.530084</td>\n",
       "      <td>0.796778</td>\n",
       "      <td>0.914716</td>\n",
       "      <td>0.273796</td>\n",
       "      <td>0.067733</td>\n",
       "      <td>...</td>\n",
       "      <td>2.742117e-08</td>\n",
       "      <td>0.144613</td>\n",
       "      <td>0.242482</td>\n",
       "      <td>0.499756</td>\n",
       "      <td>0.783107</td>\n",
       "      <td>0.922241</td>\n",
       "      <td>0.274142</td>\n",
       "      <td>0.060095</td>\n",
       "      <td>0.286990</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:20:00</th>\n",
       "      <td>2.993595e-08</td>\n",
       "      <td>0.016880</td>\n",
       "      <td>2.825645e-08</td>\n",
       "      <td>0.130407</td>\n",
       "      <td>0.261772</td>\n",
       "      <td>0.512427</td>\n",
       "      <td>0.797310</td>\n",
       "      <td>0.921577</td>\n",
       "      <td>0.249107</td>\n",
       "      <td>0.045114</td>\n",
       "      <td>...</td>\n",
       "      <td>2.813661e-08</td>\n",
       "      <td>0.141717</td>\n",
       "      <td>0.241381</td>\n",
       "      <td>0.450879</td>\n",
       "      <td>0.800429</td>\n",
       "      <td>0.920308</td>\n",
       "      <td>0.269116</td>\n",
       "      <td>0.046993</td>\n",
       "      <td>0.280625</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:22:00</th>\n",
       "      <td>5.382563e-08</td>\n",
       "      <td>0.026093</td>\n",
       "      <td>2.711694e-08</td>\n",
       "      <td>0.169723</td>\n",
       "      <td>0.320280</td>\n",
       "      <td>0.506925</td>\n",
       "      <td>0.782003</td>\n",
       "      <td>0.916003</td>\n",
       "      <td>0.315683</td>\n",
       "      <td>0.070769</td>\n",
       "      <td>...</td>\n",
       "      <td>2.869276e-08</td>\n",
       "      <td>0.173290</td>\n",
       "      <td>0.315197</td>\n",
       "      <td>0.495057</td>\n",
       "      <td>0.814955</td>\n",
       "      <td>0.917776</td>\n",
       "      <td>0.317020</td>\n",
       "      <td>0.078775</td>\n",
       "      <td>0.342454</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:24:00</th>\n",
       "      <td>5.922178e-08</td>\n",
       "      <td>0.028855</td>\n",
       "      <td>2.835084e-08</td>\n",
       "      <td>0.186603</td>\n",
       "      <td>0.286799</td>\n",
       "      <td>0.497228</td>\n",
       "      <td>0.781283</td>\n",
       "      <td>0.919718</td>\n",
       "      <td>0.343507</td>\n",
       "      <td>0.078003</td>\n",
       "      <td>...</td>\n",
       "      <td>2.695933e-08</td>\n",
       "      <td>0.114517</td>\n",
       "      <td>0.267656</td>\n",
       "      <td>0.452835</td>\n",
       "      <td>0.792680</td>\n",
       "      <td>0.919174</td>\n",
       "      <td>0.226423</td>\n",
       "      <td>0.040565</td>\n",
       "      <td>0.228638</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rb_up_var_lag-10  rb_up_lag-10  rb_down_var_lag-10  \\\n",
       "time                                                                      \n",
       "2018-03-28 16:16:00      3.143298e-08      0.014949        2.677239e-08   \n",
       "2018-03-28 16:18:00      4.439640e-08      0.023976        2.795076e-08   \n",
       "2018-03-28 16:20:00      2.993595e-08      0.016880        2.825645e-08   \n",
       "2018-03-28 16:22:00      5.382563e-08      0.026093        2.711694e-08   \n",
       "2018-03-28 16:24:00      5.922178e-08      0.028855        2.835084e-08   \n",
       "\n",
       "                     rb_down_lag-10  mcs_up_var_lag-10  mcs_up_lag-10  \\\n",
       "time                                                                    \n",
       "2018-03-28 16:16:00        0.116414           0.207425       0.483274   \n",
       "2018-03-28 16:18:00        0.145097           0.259314       0.530084   \n",
       "2018-03-28 16:20:00        0.130407           0.261772       0.512427   \n",
       "2018-03-28 16:22:00        0.169723           0.320280       0.506925   \n",
       "2018-03-28 16:24:00        0.186603           0.286799       0.497228   \n",
       "\n",
       "                     mcs_down_var_lag-10  mcs_down_lag-10  rnti_count_lag-10  \\\n",
       "time                                                                           \n",
       "2018-03-28 16:16:00             0.796265         0.929664           0.227829   \n",
       "2018-03-28 16:18:00             0.796778         0.914716           0.273796   \n",
       "2018-03-28 16:20:00             0.797310         0.921577           0.249107   \n",
       "2018-03-28 16:22:00             0.782003         0.916003           0.315683   \n",
       "2018-03-28 16:24:00             0.781283         0.919718           0.343507   \n",
       "\n",
       "                     up_lag-10  ...  rb_down_var_lag-1  rb_down_lag-1  \\\n",
       "time                            ...                                     \n",
       "2018-03-28 16:16:00   0.043207  ...       2.890976e-08       0.145582   \n",
       "2018-03-28 16:18:00   0.067733  ...       2.742117e-08       0.144613   \n",
       "2018-03-28 16:20:00   0.045114  ...       2.813661e-08       0.141717   \n",
       "2018-03-28 16:22:00   0.070769  ...       2.869276e-08       0.173290   \n",
       "2018-03-28 16:24:00   0.078003  ...       2.695933e-08       0.114517   \n",
       "\n",
       "                     mcs_up_var_lag-1  mcs_up_lag-1  mcs_down_var_lag-1  \\\n",
       "time                                                                      \n",
       "2018-03-28 16:16:00          0.232918      0.478785            0.793296   \n",
       "2018-03-28 16:18:00          0.242482      0.499756            0.783107   \n",
       "2018-03-28 16:20:00          0.241381      0.450879            0.800429   \n",
       "2018-03-28 16:22:00          0.315197      0.495057            0.814955   \n",
       "2018-03-28 16:24:00          0.267656      0.452835            0.792680   \n",
       "\n",
       "                     mcs_down_lag-1  rnti_count_lag-1  up_lag-1  down_lag-1  \\\n",
       "time                                                                          \n",
       "2018-03-28 16:16:00        0.916750          0.275640  0.051397    0.288248   \n",
       "2018-03-28 16:18:00        0.922241          0.274142  0.060095    0.286990   \n",
       "2018-03-28 16:20:00        0.920308          0.269116  0.046993    0.280625   \n",
       "2018-03-28 16:22:00        0.917776          0.317020  0.078775    0.342454   \n",
       "2018-03-28 16:24:00        0.919174          0.226423  0.040565    0.228638   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2018-03-28 16:16:00    ElBorn  \n",
       "2018-03-28 16:18:00    ElBorn  \n",
       "2018-03-28 16:20:00    ElBorn  \n",
       "2018-03-28 16:22:00    ElBorn  \n",
       "2018-03-28 16:24:00    ElBorn  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rnti_count</th>\n",
       "      <th>rb_down</th>\n",
       "      <th>rb_up</th>\n",
       "      <th>down</th>\n",
       "      <th>up</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:16:00</th>\n",
       "      <td>0.274142</td>\n",
       "      <td>0.144613</td>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.286990</td>\n",
       "      <td>0.060095</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:18:00</th>\n",
       "      <td>0.269116</td>\n",
       "      <td>0.141717</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.280625</td>\n",
       "      <td>0.046993</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:20:00</th>\n",
       "      <td>0.317020</td>\n",
       "      <td>0.173290</td>\n",
       "      <td>0.028488</td>\n",
       "      <td>0.342454</td>\n",
       "      <td>0.078775</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:22:00</th>\n",
       "      <td>0.226423</td>\n",
       "      <td>0.114517</td>\n",
       "      <td>0.016597</td>\n",
       "      <td>0.228638</td>\n",
       "      <td>0.040565</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:24:00</th>\n",
       "      <td>0.307914</td>\n",
       "      <td>0.164407</td>\n",
       "      <td>0.024169</td>\n",
       "      <td>0.327256</td>\n",
       "      <td>0.064402</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rnti_count   rb_down     rb_up      down        up  \\\n",
       "time                                                                      \n",
       "2018-03-28 16:16:00    0.274142  0.144613  0.021526  0.286990  0.060095   \n",
       "2018-03-28 16:18:00    0.269116  0.141717  0.018685  0.280625  0.046993   \n",
       "2018-03-28 16:20:00    0.317020  0.173290  0.028488  0.342454  0.078775   \n",
       "2018-03-28 16:22:00    0.226423  0.114517  0.016597  0.228638  0.040565   \n",
       "2018-03-28 16:24:00    0.307914  0.164407  0.024169  0.327256  0.064402   \n",
       "\n",
       "                    District  \n",
       "time                          \n",
       "2018-03-28 16:16:00   ElBorn  \n",
       "2018-03-28 16:18:00   ElBorn  \n",
       "2018-03-28 16:20:00   ElBorn  \n",
       "2018-03-28 16:22:00   ElBorn  \n",
       "2018-03-28 16:24:00   ElBorn  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ElBorn': MinMaxScaler(),\n",
       "  'LesCorts': MinMaxScaler(),\n",
       "  'PobleSec': MinMaxScaler()},\n",
       " {'ElBorn': MinMaxScaler(),\n",
       "  'LesCorts': MinMaxScaler(),\n",
       "  'PobleSec': MinMaxScaler()})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scalers, y_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers):\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area.\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    \n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, client_X_train, client_X_val, client_y_train, client_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scalers, y_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ElBorn': MinMaxScaler(),\n",
       "  'LesCorts': MinMaxScaler(),\n",
       "  'PobleSec': MinMaxScaler()},\n",
       " {'ElBorn': MinMaxScaler(),\n",
       "  'LesCorts': MinMaxScaler(),\n",
       "  'PobleSec': MinMaxScaler()})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scalers, y_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ElBorn', 'LesCorts', 'PobleSec'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_X_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ElBorn', 'LesCorts', 'PobleSec'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_X_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client: ElBorn\n",
      "X_train shape: (3344, 10, 11, 1), y_train shape: (3344, 5)\n",
      "X_val shape: (828, 10, 11, 1), y_val shape: (828, 5)\n",
      "\n",
      "Client: LesCorts\n",
      "X_train shape: (5504, 10, 11, 1), y_train shape: (5504, 5)\n",
      "X_val shape: (1368, 10, 11, 1), y_val shape: (1368, 5)\n",
      "\n",
      "Client: PobleSec\n",
      "X_train shape: (12732, 10, 11, 1), y_train shape: (12732, 5)\n",
      "X_val shape: (3175, 10, 11, 1), y_val shape: (3175, 5)\n"
     ]
    }
   ],
   "source": [
    "for client in client_X_train:\n",
    "    print(f\"\\nClient: {client}\")\n",
    "    print(f\"X_train shape: {client_X_train[client].shape}, y_train shape: {client_y_train[client].shape}\")\n",
    "    print(f\"X_val shape: {client_X_val[client].shape}, y_val shape: {client_y_val[client].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train):\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2]\n",
    "    else:\n",
    "        input_dim = X_train.shape[2]\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        if len(exogenous_data_train) == 1:\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1]\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1]\n",
    "    else:\n",
    "        exogenous_dim = 0\n",
    "    \n",
    "    return input_dim, exogenous_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str,\n",
    "              input_dim: int,\n",
    "              out_dim: int,\n",
    "              lags: int = 10,\n",
    "              exogenous_dim: int = 0,\n",
    "              seed=0):\n",
    "    if model == \"mlp\":\n",
    "        model = MLP(input_dim=input_dim, layer_units=[256, 128, 64], num_outputs=out_dim)\n",
    "    elif model == \"rnn\":\n",
    "        model = RNN(input_dim=input_dim, rnn_hidden_size=128, num_rnn_layers=1, rnn_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"lstm\":\n",
    "        model = LSTM(input_dim=input_dim, lstm_hidden_size=128, num_lstm_layers=1, lstm_dropout=0.0,\n",
    "                     layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"gru\":\n",
    "        model = GRU(input_dim=input_dim, gru_hidden_size=128, num_gru_layers=1, gru_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"cnn\":\n",
    "        model = CNN(num_features=input_dim, lags=lags, exogenous_dim=exogenous_dim, out_dim=out_dim)\n",
    "    elif model == \"da_encoder_decoder\":\n",
    "        model = DualAttentionAutoEncoder(input_dim=input_dim, architecture=\"lstm\", matrix_rep=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified model is not implemented. Plese define your own model or choose one from ['mlp', 'rnn', 'lstm', 'gru', 'cnn', 'da_encoder_decoder']\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "args.model_name = \"rnn\"\n",
    "\n",
    "input_dim, exogenous_dim = get_input_dims(X_train, exogenous_data_train)\n",
    "\n",
    "print(input_dim, exogenous_dim)\n",
    "\n",
    "model = get_model(model=args.model_name,\n",
    "                  input_dim=input_dim,\n",
    "                  out_dim=y_train.shape[1],\n",
    "                  lags=args.num_lags,\n",
    "                  exogenous_dim=exogenous_dim,\n",
    "                  seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(11, 128, batch_first=True)\n",
       "  (MLP_layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def prepare_local_data_loader(X_train, y_train, batch_size, shuffle=True):\n",
    "    # Assuming X_train and y_train are numpy arrays or similar\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.Tensor(X_train)  # Convert to float32 tensor\n",
    "    y_train_tensor = torch.Tensor(y_train)  # Convert to float32 tensor\n",
    "    \n",
    "    # Reshape X_train_tensor if necessary (based on your model's input requirements)\n",
    "    # For RNN, if it expects (batch, seq, feature), ensure the input is correctly shaped.\n",
    "    # X_train_tensor = X_train_tensor.reshape(-1, seq_length, num_features)\n",
    "    \n",
    "    # Create a TensorDataset\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return train_loader\n",
    "\n",
    "def fit(model, X_train, y_train, X_val, y_val, \n",
    "        exogenous_data_train=None, exogenous_data_val=None, \n",
    "        idxs=[8, 3, 1, 10, 9], # the indices of our targets in X\n",
    "        log_per=1,\n",
    "        client_creation_fn = None, # client specification\n",
    "        local_train_params=None, # local params\n",
    "        aggregation_params=None, # aggregation params\n",
    "        use_carbontracker=True\n",
    "       ):\n",
    "    # client creation definition\n",
    "    if client_creation_fn is None:\n",
    "        client_creation_fn = create_regression_client\n",
    "    # local params\n",
    "    if local_train_params is None:\n",
    "        local_train_params = {\n",
    "            \"epochs\": args.epochs, \"optimizer\": args.optimizer, \"lr\": args.lr,\n",
    "            \"criterion\": args.criterion, \"early_stopping\": args.local_early_stopping,\n",
    "            \"patience\": args.local_patience, \"device\": device\n",
    "        }\n",
    "    \n",
    "    train_loaders, val_loaders = [], []\n",
    "    \n",
    "    # get data per client\n",
    "    for client in X_train:\n",
    "        if client == \"all\":\n",
    "            continue\n",
    "        if exogenous_data_train is not None:\n",
    "            tmp_exogenous_data_train = exogenous_data_train[client]\n",
    "            tmp_exogenous_data_val = tmp_exogenous_data_val[client]\n",
    "        else:\n",
    "            tmp_exogenous_data_train = None\n",
    "            tmp_exogenous_data_val = None\n",
    "    \n",
    "        num_features = len(X_train[client][0][0])\n",
    "        \n",
    "        # to torch loader\n",
    "        train_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                X_train[client], y_train[client],\n",
    "                num_lags=args.num_lags,\n",
    "                num_features=num_features,\n",
    "                exogenous_data=tmp_exogenous_data_train,\n",
    "                indices=idxs,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "        )\n",
    "        val_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                X_val[client], y_val[client],\n",
    "                num_lags=args.num_lags,\n",
    "                exogenous_data=tmp_exogenous_data_val,\n",
    "                indices=idxs,\n",
    "                batch_size=args.batch_size,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "        )\n",
    "        \n",
    "    # create clients with their local data\n",
    "    cids = [k for k in X_train.keys() if k != \"all\"]\n",
    "    clients = [\n",
    "        client_creation_fn(\n",
    "            cid=cid, # client id\n",
    "            model=model, # the global model\n",
    "            train_loader=train_loader, # the local train loader\n",
    "            test_loader=val_loader, # the local val loader\n",
    "            local_params=local_train_params # local parameters\n",
    "        )\n",
    "        for cid, train_loader, val_loader in zip(cids, train_loaders, val_loaders)\n",
    "    ]\n",
    "    \n",
    "    # represent clients to server\n",
    "    client_proxies = [\n",
    "        SimpleClientProxy(cid, client) for cid, client in zip(cids, clients)\n",
    "    ]\n",
    "    \n",
    "    # represent the server\n",
    "    server = Server(\n",
    "        client_proxies=client_proxies, # the client representations\n",
    "        aggregation=args.aggregation, # the aggregation algorithm\n",
    "        aggregation_params=aggregation_params, # aggregation specific params\n",
    "        local_params_fn=None, # we can change the local params on demand\n",
    "    )\n",
    "    # Note that the client manager instance will be initialized automatically. You can define your own client manager.\n",
    "\n",
    "    # train with FL\n",
    "    model_params, history = server.fit(args.fl_rounds, args.fraction, use_carbontracker=use_carbontracker)\n",
    "    \n",
    "\n",
    "    for cid, client_proxy in zip(cids, client_proxies):\n",
    "\n",
    "        fine_tune_params = {\n",
    "            \"epochs\": 2,\n",
    "            \"lr\": args.lr * 0.1,\n",
    "            \"batch_size\": args.batch_size,\n",
    "        }\n",
    "        client = client_proxy.client\n",
    "        local_model = client.net\n",
    "\n",
    "        local_train_loader = prepare_local_data_loader(X_train=client_X_train[cid], y_train=client_y_train[cid],\n",
    "                                                    batch_size=fine_tune_params['batch_size'], shuffle=True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=fine_tune_params['lr'])\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        local_model.train()\n",
    "        for epoch in range(fine_tune_params['epochs']):\n",
    "            for data, targets in local_train_loader:\n",
    "                data, targets = data.to(device), targets.to(device)  # Move data to the correct device\n",
    "                optimizer.zero_grad()\n",
    "                outputs = local_model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Client {cid}: Epoch {epoch+1}/{fine_tune_params['epochs']}, Loss: {loss.item()}\")\n",
    "        \n",
    "    params_dict = zip(model.state_dict().keys(), model_params)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    model = copy.deepcopy(model)\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated local params\n",
    "local_train_params = {\"epochs\": args.epochs, \"optimizer\": args.optimizer, \"lr\": args.lr,\n",
    "                      \"criterion\": args.criterion, \"early_stopping\": args.local_early_stopping,\n",
    "                      \"patience\": args.local_patience, \"device\": device\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-02-09 21:50:32,224 | server.py:62 | Initializing client manager...\n",
      "INFO logger 2024-02-09 21:50:32,224 | server.py:69 | Registering clients...\n",
      "INFO logger 2024-02-09 21:50:32,225 | client_manager.py:66 | Registered client with id: ElBorn\n",
      "INFO logger 2024-02-09 21:50:32,225 | client_manager.py:66 | Registered client with id: LesCorts\n",
      "INFO logger 2024-02-09 21:50:32,225 | client_manager.py:66 | Registered client with id: PobleSec\n",
      "INFO logger 2024-02-09 21:50:32,225 | server.py:73 | Client manager initialized!\n",
      "INFO logger 2024-02-09 21:50:32,226 | server.py:55 | Aggregation algorithm: FedAvg()\n",
      "INFO logger 2024-02-09 21:50:32,226 | client_manager.py:88 | Parameter c=0.0. Sampled 1 client(s): ['LesCorts']\n",
      "INFO logger 2024-02-09 21:50:32,756 | server.py:86 | Starting FL rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarbonTracker: The following components were found: GPU with device(s) GPU, ANE. CPU with device(s) CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2024-02-09 21:50:33,947 | client_manager.py:88 | Parameter c=1.0. Sampled 3 client(s): ['LesCorts', 'ElBorn', 'PobleSec']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarbonTracker: CRITICAL - Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/carbontracker/tracker.py\", line 122, in run\n",
      "    self._collect_measurements()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/carbontracker/tracker.py\", line 201, in _collect_measurements\n",
      "    comp.collect_power_usage(self.epoch_counter)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/carbontracker/components/component.py\", line 84, in collect_power_usage\n",
      "    self.power_usages[-1].append(self.handler.power_usage())\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/carbontracker/components/apple_silicon/powermetrics.py\", line 65, in power_usage\n",
      "    output = PowerMetricsUnified.get_output()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/carbontracker/components/apple_silicon/powermetrics.py\", line 15, in get_output\n",
      "    PowerMetricsUnified._output = subprocess.check_output(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py\", line 466, in check_output\n",
      "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py\", line 571, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['sudo', 'powermetrics', '-n', '1', '-i', '1000', '--samplers', 'all']' returned non-zero exit status 1.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "global_model, history = fit(\n",
    "    model,\n",
    "    client_X_train,\n",
    "    client_y_train, \n",
    "    client_X_val, \n",
    "    client_y_val, \n",
    "    local_train_params=local_train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_preds(y_pred_train, y_pred_val):\n",
    "    if not isinstance(y_pred_train, np.ndarray):\n",
    "        y_pred_train = y_pred_train.cpu().numpy()\n",
    "    if not isinstance(y_pred_val, np.ndarray):\n",
    "        y_pred_val = y_pred_val.cpu().numpy()\n",
    "    return y_pred_train, y_pred_val\n",
    "\n",
    "def round_predictions(y_pred_train, y_pred_val, dims):\n",
    "    # round to closest integer\n",
    "    if dims is None or len(dims) == 0:\n",
    "        return y_pred_train, y_pred_val\n",
    "    for dim in dims:\n",
    "        y_pred_train[:, dim] = np.rint(y_pred_train[:, dim])\n",
    "        y_pred_val[:, dim] = np.rint(y_pred_val[:, dim])\n",
    "    return y_pred_train, y_pred_val\n",
    "\n",
    "def inverse_transform(y_train, y_val, y_pred_train, y_pred_val,\n",
    "                     y_scaler=None, \n",
    "                     round_preds=False, dims=None):\n",
    "    y_pred_train, y_pred_val = transform_preds(y_pred_train, y_pred_val)\n",
    "    \n",
    "    if y_scaler is not None:\n",
    "        y_train = y_scaler.inverse_transform(y_train)\n",
    "        y_val = y_scaler.inverse_transform(y_val)\n",
    "        y_pred_train = y_scaler.inverse_transform(y_pred_train)\n",
    "        y_pred_val = y_scaler.inverse_transform(y_pred_val)\n",
    "    \n",
    "    # to zeroes\n",
    "    y_pred_train[y_pred_train < 0.] = 0.\n",
    "    y_pred_val[y_pred_val < 0.] = 0.\n",
    "    \n",
    "    if round_preds:\n",
    "        y_pred_train, y_pred_val = round_predictions(y_pred_train, y_pred_val, dims)\n",
    "    \n",
    "    return y_train, y_val, y_pred_train, y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(y_true, y_pred, \n",
    "              title, \n",
    "              feature_names=None, \n",
    "              client=None):\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(y_pred.shape[1])]\n",
    "    assert len(feature_names) == y_pred.shape[1]\n",
    "\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.ticklabel_format(style='plain')\n",
    "        plt.plot(y_true[:, i], label=\"Actual\")\n",
    "        plt.plot(y_pred[:, i], label=\"Predicted\")\n",
    "        if client is not None:\n",
    "            plt.title(f\"[{client} {title}] {feature_names[i]} prediction\")\n",
    "        else:\n",
    "            plt.title(f\"[{title}] {feature_names[i]} prediction\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    model, # the global model\n",
    "    client_X_train, # train data per client\n",
    "    client_y_train,\n",
    "    client_X_val, # val data per client\n",
    "    client_y_val,\n",
    "    exogenous_data_train, # exogenous data per client\n",
    "    exogenous_data_val,\n",
    "    y_scalers, # the scalers used to transform the targets\n",
    "    idxs=[8, 3, 1, 10, 9],\n",
    "    apply_round=True, # round to closest integer\n",
    "    round_dimensions=[0, 3, 4], # the dimensions to apply rounding\n",
    "    plot=True, # plot predictions\n",
    "):\n",
    "    # load per client data to torch\n",
    "    train_loaders, val_loaders = [], []\n",
    "    \n",
    "    # get data per client\n",
    "    for client in client_X_train:\n",
    "        if client == \"all\":\n",
    "            continue\n",
    "        assert client in list(y_scalers.keys())\n",
    "        if exogenous_data_train is not None:\n",
    "            tmp_exogenous_data_train = exogenous_data_train[client]\n",
    "            tmp_exogenous_data_val = exogenous_data_val[client]\n",
    "        else:\n",
    "            tmp_exogenous_data_train = None\n",
    "            tmp_exogenous_data_val = None\n",
    "    \n",
    "        num_features = len(client_X_train[client][0][0])\n",
    "        \n",
    "        # to torch loader\n",
    "        train_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                client_X_train[client], client_y_train[client],\n",
    "                num_lags=args.num_lags,\n",
    "                num_features=num_features,\n",
    "                exogenous_data=tmp_exogenous_data_train,\n",
    "                indices=idxs,\n",
    "                batch_size=1,\n",
    "                shuffle=False\n",
    "            )\n",
    "        )\n",
    "        val_loaders.append(\n",
    "            to_torch_dataset(\n",
    "                client_X_val[client], client_y_val[client],\n",
    "                num_lags=args.num_lags,\n",
    "                exogenous_data=tmp_exogenous_data_val,\n",
    "                indices=idxs,\n",
    "                batch_size=1,\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "        )\n",
    "        \n",
    "    # get client ids\n",
    "    cids = [k for k in client_X_train.keys() if k != \"all\"]\n",
    "        \n",
    "    # predict per client using the global model\n",
    "    y_preds_train, y_preds_val = dict(), dict()\n",
    "    for cid, train_loader, val_loader in zip(cids, train_loaders, val_loaders):\n",
    "        print(f\"Prediction on {cid}\")\n",
    "        train_mse, train_rmse, train_mae, train_r2, train_nrmse, y_pred_train = test(\n",
    "            model, train_loader, None, device=device\n",
    "        )\n",
    "        val_mse, val_rmse, val_mae, val_r2, val_nrmse, y_pred_val = test(\n",
    "            model, val_loader, None, device=device\n",
    "        )\n",
    "        y_preds_train[cid] = y_pred_train\n",
    "        y_preds_val[cid] = y_pred_val\n",
    "    \n",
    "    for cid in cids:\n",
    "        y_train, y_val = client_y_train[cid], client_y_val[cid]\n",
    "        y_pred_train, y_pred_val = y_preds_train[cid], y_preds_val[cid]\n",
    "        \n",
    "        y_scaler = y_scalers[cid]\n",
    "        y_train, y_val, y_pred_train, y_pred_val = inverse_transform(\n",
    "            y_train, y_val, y_pred_train, y_pred_val,\n",
    "            y_scaler, round_preds=apply_round, dims=round_dimensions\n",
    "        )\n",
    "        train_mse, train_rmse, train_mae, train_r2, train_nrmse, train_res_per_dim = accumulate_metric(\n",
    "            y_train, y_pred_train, True, return_all=True\n",
    "        )\n",
    "        val_mse, val_rmse, val_mae, val_r2, val_nrmse, val_res_per_dim = accumulate_metric(\n",
    "            y_val, y_pred_val, True, return_all=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFinal Prediction on {cid} (Inference Stage)\")\n",
    "        print(f\"[Train]: mse: {train_mse}, \"\n",
    "              f\"rmse: {train_rmse}, mae {train_mae}, r2: {train_r2}, nrmse: {train_nrmse}\")\n",
    "        print(f\"[Val]: mse: {val_mse}, \"\n",
    "              f\"rmse: {val_rmse}, mae {val_mae}, r2: {val_r2}, nrmse: {val_nrmse}\\n\\n\")\n",
    "        \n",
    "        if plot:\n",
    "            make_plot(y_train, y_pred_train, title=\"Train\", feature_names=args.targets, client=cid)\n",
    "            make_plot(y_val, y_pred_val, title=\"Val\", feature_names=args.targets, client=cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(\n",
    "    global_model,\n",
    "    client_X_train, \n",
    "    client_y_train,\n",
    "    client_X_val, \n",
    "    client_y_val,\n",
    "    exogenous_data_train, \n",
    "    exogenous_data_val,\n",
    "    y_scalers\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
